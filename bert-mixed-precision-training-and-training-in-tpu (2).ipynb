{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9417219,"sourceType":"datasetVersion","datasetId":5719327},{"sourceId":9437079,"sourceType":"datasetVersion","datasetId":5734257},{"sourceId":116960,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":98323,"modelId":122502}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-19T19:10:14.064196Z","iopub.execute_input":"2024-09-19T19:10:14.064740Z","iopub.status.idle":"2024-09-19T19:10:14.540759Z","shell.execute_reply.started":"2024-09-19T19:10:14.064709Z","shell.execute_reply":"2024-09-19T19:10:14.539999Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/uuuu/pytorch/default/1/CoreDischarge Model_epoch_4roc_0.7123073312793169.pth\n/kaggle/input/los-ad/LOS/LOS_WEEKS_adm_val.csv\n/kaggle/input/los-ad/LOS/LOS_WEEKS_adm_train.csv\n/kaggle/input/los-ad/LOS/LOS_WEEKS_adm_test.csv\n/kaggle/input/bert-of/LOS/LOS_WEEKS_adm_val.csv\n/kaggle/input/bert-of/LOS/LOS_WEEKS_adm_train.csv\n/kaggle/input/bert-of/LOS/LOS_WEEKS_adm_test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom transformers import BertForSequenceClassification, BertTokenizer, AdamW, get_linear_schedule_with_warmup\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoConfig\nfrom torch.utils.data import DataLoader\nfrom torch import nn\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom tqdm import tqdm\nimport os\nfrom torch.nn import functional as F\nimport torch.quantization\nimport torch.nn.utils.prune as prune","metadata":{"execution":{"iopub.status.busy":"2024-09-19T19:10:14.542109Z","iopub.execute_input":"2024-09-19T19:10:14.542501Z","iopub.status.idle":"2024-09-19T19:10:22.631691Z","shell.execute_reply.started":"2024-09-19T19:10:14.542473Z","shell.execute_reply":"2024-09-19T19:10:22.630881Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/usr/local/lib/python3.10/site-packages/torch_xla/__init__.py:202: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n  warnings.warn(\nWARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1726773019.701679    1572 common_lib.cc:798] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: ===\nlearning/45eac/tfrc/runtime/common_lib.cc:479\nE0919 19:10:19.753869236    1572 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2024-09-19T19:10:19.753851448+00:00\", grpc_status:2}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Loading Dataset","metadata":{}},{"cell_type":"code","source":"train=pd.read_csv(\"/kaggle/input/bert-of/LOS/LOS_WEEKS_adm_train.csv\")\ntest=pd.read_csv(\"/kaggle/input/bert-of/LOS/LOS_WEEKS_adm_test.csv\")\nval=pd.read_csv(\"/kaggle/input/bert-of/LOS/LOS_WEEKS_adm_val.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-09-19T19:10:22.632621Z","iopub.execute_input":"2024-09-19T19:10:22.633078Z","iopub.status.idle":"2024-09-19T19:10:23.657222Z","shell.execute_reply.started":"2024-09-19T19:10:22.633049Z","shell.execute_reply":"2024-09-19T19:10:23.656529Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#train['los_label'][:5000].tolist()","metadata":{"execution":{"iopub.status.busy":"2024-09-19T19:10:23.658165Z","iopub.execute_input":"2024-09-19T19:10:23.658433Z","iopub.status.idle":"2024-09-19T19:10:23.662018Z","shell.execute_reply.started":"2024-09-19T19:10:23.658407Z","shell.execute_reply":"2024-09-19T19:10:23.661272Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Maximum Length of a note","metadata":{}},{"cell_type":"code","source":"max(len(x) for x in train['text'])","metadata":{"execution":{"iopub.status.busy":"2024-09-19T19:10:23.663991Z","iopub.execute_input":"2024-09-19T19:10:23.664485Z","iopub.status.idle":"2024-09-19T19:10:23.686410Z","shell.execute_reply.started":"2024-09-19T19:10:23.664458Z","shell.execute_reply":"2024-09-19T19:10:23.685716Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"22431"},"metadata":{}}]},{"cell_type":"markdown","source":"# Avg Length of a Note","metadata":{}},{"cell_type":"code","source":"import statistics\nstatistics.mean([len(x) for x in train['text']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model HyperParameters","metadata":{}},{"cell_type":"code","source":"batch_size = 8\nmax_tokens = 512\nepochs = 200\nbest_roc_auc = 0.0\nmin_delta = 0.0001\nearly_stopping_count = 0\nearly_stopping_patience = 3\ngradient_accumulation_steps = 10\nlr = 1e-5\ntemperature = 2.0\nweight_decay = 0.01\nnum_warmup_steps = 0\npruning_ratio = 0.3\ndropout_prob = 0.2","metadata":{"execution":{"iopub.status.busy":"2024-09-19T19:10:23.725178Z","iopub.execute_input":"2024-09-19T19:10:23.725425Z","iopub.status.idle":"2024-09-19T19:10:23.729821Z","shell.execute_reply.started":"2024-09-19T19:10:23.725400Z","shell.execute_reply":"2024-09-19T19:10:23.729096Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Make Ensemble","metadata":{}},{"cell_type":"code","source":"class EnsembleModel(nn.Module):\n    def __init__(self, model1, model2):\n        super(EnsembleModel, self).__init__()\n        self.model1 = model1\n        self.model2 = model2\n\n    def forward(self, input_ids, attention_mask):\n        output1 = self.model1(input_ids, attention_mask=attention_mask)[0]\n        output2 = self.model2(input_ids, attention_mask=attention_mask)[0]\n        avg_output = (output1 + output2) / 2.00\n        return avg_output","metadata":{"execution":{"iopub.status.busy":"2024-09-19T19:10:23.730685Z","iopub.execute_input":"2024-09-19T19:10:23.730973Z","iopub.status.idle":"2024-09-19T19:10:23.738635Z","shell.execute_reply.started":"2024-09-19T19:10:23.730929Z","shell.execute_reply":"2024-09-19T19:10:23.737993Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Load Model From Hugging Face","metadata":{}},{"cell_type":"code","source":"config = AutoConfig.from_pretrained('bvanaken/CORe-clinical-outcome-biobert-v1',\n                                    num_labels=4,\n                                    hidden_dropout_prob=dropout_prob,\n                                    attention_probs_dropout_prob=dropout_prob)\ncore_model = AutoModelForSequenceClassification.from_pretrained('bvanaken/CORe-clinical-outcome-biobert-v1' ,config=config)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T19:10:23.739438Z","iopub.execute_input":"2024-09-19T19:10:23.739684Z","iopub.status.idle":"2024-09-19T19:10:24.741233Z","shell.execute_reply.started":"2024-09-19T19:10:23.739658Z","shell.execute_reply":"2024-09-19T19:10:24.740249Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bvanaken/CORe-clinical-outcome-biobert-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"config = AutoConfig.from_pretrained('emilyalsentzer/Bio_Discharge_Summary_BERT',\n                                    num_labels=4,\n                                    hidden_dropout_prob=dropout_prob,\n                                    attention_probs_dropout_prob=dropout_prob)\n\n# Load the pre-trained model with the specified configuration\ndischarge_model = AutoModelForSequenceClassification.from_pretrained('emilyalsentzer/Bio_Discharge_Summary_BERT', config=config)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T19:10:24.742397Z","iopub.execute_input":"2024-09-19T19:10:24.742661Z","iopub.status.idle":"2024-09-19T19:10:25.400132Z","shell.execute_reply.started":"2024-09-19T19:10:24.742634Z","shell.execute_reply":"2024-09-19T19:10:25.399146Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_Discharge_Summary_BERT and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Push Model To device (\"Tpu\")","metadata":{}},{"cell_type":"markdown","source":"> **Use Cuda For Mixed Precision Training** ","metadata":{}},{"cell_type":"code","source":"#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    ^^^For MIXED PRECISION \nimport torch_xla\nimport torch_xla.core.xla_model as xm\n\n# Check if TPU is available\ndevice = xm.xla_device()\nprint(device)\n\nensemble_model = EnsembleModel(core_model, discharge_model)\nensemble_model = ensemble_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T19:10:25.401345Z","iopub.execute_input":"2024-09-19T19:10:25.401650Z","iopub.status.idle":"2024-09-19T19:10:28.777520Z","shell.execute_reply.started":"2024-09-19T19:10:25.401624Z","shell.execute_reply":"2024-09-19T19:10:28.776413Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1726773025.450449    1572 common_lib.cc:818] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: === \nlearning/45eac/tfrc/runtime/common_lib.cc:483\n","output_type":"stream"},{"name":"stdout","text":"xla:0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Creating custom dataset","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('bvanaken/CORe-clinical-outcome-biobert-v1')","metadata":{"execution":{"iopub.status.busy":"2024-09-19T19:10:28.778632Z","iopub.execute_input":"2024-09-19T19:10:28.778924Z","iopub.status.idle":"2024-09-19T19:10:29.087366Z","shell.execute_reply.started":"2024-09-19T19:10:28.778895Z","shell.execute_reply":"2024-09-19T19:10:29.086229Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Dataset Class","metadata":{}},{"cell_type":"code","source":"class LosDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T19:10:29.090503Z","iopub.execute_input":"2024-09-19T19:10:29.090817Z","iopub.status.idle":"2024-09-19T19:10:29.096420Z","shell.execute_reply.started":"2024-09-19T19:10:29.090786Z","shell.execute_reply":"2024-09-19T19:10:29.095474Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Tokenization","metadata":{}},{"cell_type":"code","source":"train_encodings = tokenizer(train['text'].tolist(),truncation=True, padding=True, max_length = max_tokens)                       #Expects list[str] so train['text'].tolist()\nval_encodings = tokenizer(val['text'].tolist(), truncation=True, padding=True,  max_length = max_tokens)\ntest_encodings = tokenizer(test['text'].tolist(), truncation=True, padding=True , max_length = max_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T19:10:29.097293Z","iopub.execute_input":"2024-09-19T19:10:29.097524Z","iopub.status.idle":"2024-09-19T19:10:41.038477Z","shell.execute_reply.started":"2024-09-19T19:10:29.097500Z","shell.execute_reply":"2024-09-19T19:10:41.037187Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_dataset = LosDataset(train_encodings, train['los_label'].tolist())\nval_dataset = LosDataset(val_encodings, val['los_label'].tolist())\ntest_dataset = LosDataset(test_encodings, test['los_label'].tolist())","metadata":{"execution":{"iopub.status.busy":"2024-09-19T19:10:41.039578Z","iopub.execute_input":"2024-09-19T19:10:41.039879Z","iopub.status.idle":"2024-09-19T19:10:41.045167Z","shell.execute_reply.started":"2024-09-19T19:10:41.039843Z","shell.execute_reply":"2024-09-19T19:10:41.044366Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"len(train_dataset.labels[:])","metadata":{"execution":{"iopub.status.busy":"2024-09-19T19:10:41.046072Z","iopub.execute_input":"2024-09-19T19:10:41.046306Z","iopub.status.idle":"2024-09-19T19:10:41.056997Z","shell.execute_reply.started":"2024-09-19T19:10:41.046281Z","shell.execute_reply":"2024-09-19T19:10:41.056302Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"30421"},"metadata":{}}]},{"cell_type":"markdown","source":"## Loading Model to Dataloader ","metadata":{}},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T19:10:41.057759Z","iopub.execute_input":"2024-09-19T19:10:41.057999Z","iopub.status.idle":"2024-09-19T19:10:41.065371Z","shell.execute_reply.started":"2024-09-19T19:10:41.057975Z","shell.execute_reply":"2024-09-19T19:10:41.064596Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Setting Up Optimizer And Scheduler","metadata":{}},{"cell_type":"code","source":"optimizer = AdamW(ensemble_model.parameters(), lr=lr, weight_decay=weight_decay)\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=num_warmup_steps,\n    num_training_steps=len(train_loader) * epochs // gradient_accumulation_steps\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T19:10:41.066305Z","iopub.execute_input":"2024-09-19T19:10:41.066538Z","iopub.status.idle":"2024-09-19T19:10:41.078641Z","shell.execute_reply.started":"2024-09-19T19:10:41.066515Z","shell.execute_reply":"2024-09-19T19:10:41.077819Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Loading Trained Model's state dict  (IF NEEDED)","metadata":{}},{"cell_type":"markdown","source":"### FOR MIXED PRECISION TRANING","metadata":{}},{"cell_type":"code","source":"state_dict = torch.load('/kaggle/input/uuuu/pytorch/default/1/CoreDischarge Model_epoch_4roc_0.7123073312793169.pth')                 \n# Load the state dict into your model\nensemble_model.load_state_dict(state_dict)\nensemble_model = ensemble_model.to(device) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### FOR MULTI CORE TPU","metadata":{}},{"cell_type":"code","source":"state_dict = torch.load('/kaggle/input/uuuu/pytorch/default/1/CoreDischarge Model_epoch_4roc_0.7123073312793169.pth',map_location=torch.device('cpu')) # Pushing to Cpu\ndevice = xm.xla_device()                  \n\n# Load the state dict into your model\nensemble_model.load_state_dict(state_dict)\nensemble_model = ensemble_model.to(device)   # Pushing to TPU","metadata":{"execution":{"iopub.status.busy":"2024-09-19T19:10:41.079574Z","iopub.execute_input":"2024-09-19T19:10:41.079804Z","iopub.status.idle":"2024-09-19T19:10:42.286946Z","shell.execute_reply.started":"2024-09-19T19:10:41.079782Z","shell.execute_reply":"2024-09-19T19:10:42.285886Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_1572/3636163748.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load('/kaggle/input/uuuu/pytorch/default/1/CoreDischarge Model_epoch_4roc_0.7123073312793169.pth',map_location=torch.device('cpu'))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Single Train Loader","metadata":{}},{"cell_type":"code","source":"for step,batch in enumerate(train_loader):\n    print(step,end=\"\\n \\n\")\n    print(batch)\n    \n    if step==0:\n        break","metadata":{"execution":{"iopub.status.busy":"2024-09-19T19:10:42.288127Z","iopub.execute_input":"2024-09-19T19:10:42.288406Z","iopub.status.idle":"2024-09-19T19:10:42.304982Z","shell.execute_reply.started":"2024-09-19T19:10:42.288378Z","shell.execute_reply":"2024-09-19T19:10:42.304128Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"0\n \n{'input_ids': tensor([[  101,  2705, 12522,  ...,  1113,  4870,   102],\n        [  101,  2705, 12522,  ..., 19310,   131,   102],\n        [  101,  2705, 12522,  ...,     0,     0,     0],\n        ...,\n        [  101,  2705, 12522,  ...,     0,     0,     0],\n        [  101,  2705, 12522,  ...,   120,   185,   102],\n        [  101,  2705, 12522,  ...,   117,  2999,   102]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        ...,\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([0, 2, 3, 1, 2, 0, 3, 2])}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Train and Eval Loop\n","metadata":{}},{"cell_type":"code","source":"ensemble_name='Core+Discharge Model'\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-09-19T19:10:42.305911Z","iopub.execute_input":"2024-09-19T19:10:42.306173Z","iopub.status.idle":"2024-09-19T19:10:42.309647Z","shell.execute_reply.started":"2024-09-19T19:10:42.306149Z","shell.execute_reply":"2024-09-19T19:10:42.308876Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Dependiencies for TPU","metadata":{}},{"cell_type":"code","source":"import torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.utils.utils as xu\nimport torch_xla.core.xla_model as xm\nimport torch_xla.core.functions as xf","metadata":{"execution":{"iopub.status.busy":"2024-09-19T19:10:42.310486Z","iopub.execute_input":"2024-09-19T19:10:42.310705Z","iopub.status.idle":"2024-09-19T19:10:42.319707Z","shell.execute_reply.started":"2024-09-19T19:10:42.310682Z","shell.execute_reply":"2024-09-19T19:10:42.318997Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### Model Parameters","metadata":{}},{"cell_type":"code","source":"ensemble_model.parameters","metadata":{"execution":{"iopub.status.busy":"2024-09-19T19:10:42.320566Z","iopub.execute_input":"2024-09-19T19:10:42.320788Z","iopub.status.idle":"2024-09-19T19:10:42.331209Z","shell.execute_reply.started":"2024-09-19T19:10:42.320761Z","shell.execute_reply":"2024-09-19T19:10:42.330380Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"<bound method Module.parameters of EnsembleModel(\n  (model1): BertForSequenceClassification(\n    (bert): BertModel(\n      (embeddings): BertEmbeddings(\n        (word_embeddings): Embedding(28996, 768, padding_idx=0)\n        (position_embeddings): Embedding(512, 768)\n        (token_type_embeddings): Embedding(2, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.2, inplace=False)\n      )\n      (encoder): BertEncoder(\n        (layer): ModuleList(\n          (0-11): 12 x BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.2, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.2, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n        )\n      )\n      (pooler): BertPooler(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (activation): Tanh()\n      )\n    )\n    (dropout): Dropout(p=0.2, inplace=False)\n    (classifier): Linear(in_features=768, out_features=4, bias=True)\n  )\n  (model2): BertForSequenceClassification(\n    (bert): BertModel(\n      (embeddings): BertEmbeddings(\n        (word_embeddings): Embedding(28996, 768, padding_idx=0)\n        (position_embeddings): Embedding(512, 768)\n        (token_type_embeddings): Embedding(2, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.2, inplace=False)\n      )\n      (encoder): BertEncoder(\n        (layer): ModuleList(\n          (0-11): 12 x BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.2, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.2, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n        )\n      )\n      (pooler): BertPooler(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (activation): Tanh()\n      )\n    )\n    (dropout): Dropout(p=0.2, inplace=False)\n    (classifier): Linear(in_features=768, out_features=4, bias=True)\n  )\n)>"},"metadata":{}}]},{"cell_type":"markdown","source":"### See If Model loaded on Tpu","metadata":{}},{"cell_type":"code","source":"param_device = next(ensemble_model.parameters()).device\nprint(f\"Model is loaded on device: {param_device}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-19T19:10:42.332265Z","iopub.execute_input":"2024-09-19T19:10:42.332538Z","iopub.status.idle":"2024-09-19T19:10:42.339275Z","shell.execute_reply.started":"2024-09-19T19:10:42.332511Z","shell.execute_reply":"2024-09-19T19:10:42.338471Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Model is loaded on device: xla:0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Using Multi Core  TPU for Training**","metadata":{}},{"cell_type":"code","source":"for epoch in range(epochs):\n    ensemble_model.train()\n    train_loss = 0\n    para_loader = pl.MpDeviceLoader(train_loader, device)  #~~~~~~~~~~~~~~~~~~~~~  HERE\n    for step, batch in enumerate(tqdm(para_loader)):       #~~~~~~~~~~~~~~~~~~~~~  HERE\n        optimizer.zero_grad() if step % gradient_accumulation_steps == 0 else None\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        outputs = ensemble_model(input_ids, attention_mask)\n        loss = nn.CrossEntropyLoss()(outputs, labels)\n        (loss / gradient_accumulation_steps).backward()\n        train_loss += loss.item()\n        if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(train_loader):\n            xm.optimizer_step(optimizer)\n            scheduler.step()\n\n    ensemble_model.eval()\n    val_loss = 0\n    val_preds = []\n    val_labels = []\n    para_val_loader = pl.MpDeviceLoader(val_loader, device)       #~~~~~~~~~~~~~~~~~~~~~  HERE\n    with torch.no_grad():\n        for batch in tqdm(para_val_loader):                       #~~~~~~~~~~~~~~~~~~~~~  HERE\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            outputs = ensemble_model(input_ids, attention_mask)\n            loss = nn.CrossEntropyLoss()(outputs, labels)\n            val_loss += loss.item()\n            val_preds.append(F.softmax(outputs, dim=1).cpu().numpy())\n            val_labels.append(labels.cpu().numpy())\n\n\n    val_preds = np.concatenate(val_preds)\n    val_labels = np.concatenate(val_labels)\n    val_loss /= len(val_loader)\n    train_loss /= len(train_loader)\n    print(f'Epoch: {epoch+1}/{epochs}, Training Loss: {train_loss}, Validation Loss: {val_loss}')\n\n    # Calculate metrics\n    val_preds_class = np.argmax(val_preds, axis=1)\n    accuracy = accuracy_score(val_labels, val_preds_class)\n    recall = recall_score(val_labels, val_preds_class, average='weighted')\n    precision = precision_score(val_labels, val_preds_class, average='weighted')\n    f1 = f1_score(val_labels, val_preds_class, average='weighted')\n    micro_f1 = f1_score(val_labels, val_preds_class, average='micro')\n    macro_roc_auc = roc_auc_score(val_labels, val_preds, multi_class='ovo', average='macro')\n\n    print(f'Accuracy: {accuracy}, Recall: {recall}, Precision: {precision}, F1: {f1}, Micro F1: {micro_f1}, Macro Roc Auc: {macro_roc_auc}')\n\n\n    # Implement early stopping\n    if macro_roc_auc - best_roc_auc < min_delta:\n        early_stopping_count += 1\n        print(f'EarlyStopping counter: {early_stopping_count} out of {early_stopping_patience}')\n        if early_stopping_count >= early_stopping_patience:\n            print('Early stopping')\n            break\n    else:\n        best_roc_auc = macro_roc_auc\n        early_stopping_count = 0\n        xm.save(ensemble_model.state_dict(), f\"{ensemble_name}_epoch_{epoch}roc_{best_roc_auc}.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-09-19T19:10:42.340247Z","iopub.execute_input":"2024-09-19T19:10:42.340489Z","iopub.status.idle":"2024-09-19T21:09:02.939642Z","shell.execute_reply.started":"2024-09-19T19:10:42.340466Z","shell.execute_reply":"2024-09-19T21:09:02.938175Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"100%|██████████| 3803/3803 [30:02<00:00,  2.11it/s] \n100%|██████████| 549/549 [02:21<00:00,  3.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 1/200, Training Loss: 1.1207286196729243, Validation Loss: 1.165790316077095\nAccuracy: 0.43976315190161697, Recall: 0.439763151901617, Precision: 0.4446875419560157, F1: 0.4272680911871791, Micro F1: 0.43976315190161697, Macro Roc Auc: 0.7149864563198599\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3803/3803 [26:46<00:00,  2.37it/s]\n100%|██████████| 549/549 [01:45<00:00,  5.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 2/200, Training Loss: 1.0965644086375852, Validation Loss: 1.1962702908368277\nAccuracy: 0.440218629013892, Recall: 0.440218629013892, Precision: 0.45028747894311627, F1: 0.4305616015178639, Micro F1: 0.440218629013892, Macro Roc Auc: 0.7108677777112343\nEarlyStopping counter: 1 out of 3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3803/3803 [26:48<00:00,  2.36it/s]\n100%|██████████| 549/549 [01:47<00:00,  5.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 3/200, Training Loss: 1.065258313645321, Validation Loss: 1.1891659902093188\nAccuracy: 0.42951491687542703, Recall: 0.42951491687542703, Precision: 0.4272291606057163, F1: 0.42020075082331787, Micro F1: 0.42951491687542703, Macro Roc Auc: 0.7112787973661283\nEarlyStopping counter: 2 out of 3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3803/3803 [26:58<00:00,  2.35it/s]\n100%|██████████| 549/549 [01:46<00:00,  5.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 4/200, Training Loss: 1.0343283285302862, Validation Loss: 1.204501924239004\nAccuracy: 0.4345251651104532, Recall: 0.4345251651104532, Precision: 0.4380275976826215, F1: 0.42467038858247114, Micro F1: 0.4345251651104532, Macro Roc Auc: 0.7117125421949982\nEarlyStopping counter: 3 out of 3\nEarly stopping\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Using Mixed Precision Technique for Training**","metadata":{}},{"cell_type":"markdown","source":"### *Use Cuda for this*","metadata":{}},{"cell_type":"code","source":"from torch.cuda.amp import autocast, GradScaler # IMPORTS\n\n\nscaler = torch.amp.GradScaler(\"cuda\")          #~~~~~~~~~~~~~~~~~~~~~  HERE\n\nfor epoch in range(epochs):\n    \n    ensemble_model.train()\n    train_loss = 0\n    for step, batch in enumerate(tqdm(train_loader)):\n        optimizer.zero_grad() if step % gradient_accumulation_steps == 0 else None\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        # Use autocast for mixed precision\n        with torch.amp.autocast(\"cuda\"):        #~~~~~~~~~~~~~~~~~~~~~  HERE\n            outputs = ensemble_model(input_ids, attention_mask)\n            loss = nn.CrossEntropyLoss()(outputs, labels)\n            loss = loss / gradient_accumulation_steps\n\n        # Scale the loss and call backward()\n        scaler.scale(loss).backward()          #~~~~~~~~~~~~~~~~~~~~~  HERE\n        train_loss += loss.item()\n\n        if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(train_loader):\n            # Unscales the gradients of optimizer's assigned params in-place\n            scaler.unscale_(optimizer)\n            \n            # Clip gradients\n            torch.nn.utils.clip_grad_norm_(ensemble_model.parameters(), max_norm=1.0)\n            \n            # Optimizer step \n            scaler.step(optimizer)            #~~~~~~~~~~~~~~~~~~~~~  HERE\n            \n            # Update the scale for next iteration\n            scaler.update()\n            scheduler.step()\n            \n\n    ensemble_model.eval()\n    val_loss = 0\n    val_preds = []\n    val_labels = []\n    with torch.no_grad():\n        for batch in tqdm(val_loader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            #with torch.amp.autocast(\"cuda\"):\n            outputs = ensemble_model(input_ids, attention_mask)\n            loss = nn.CrossEntropyLoss()(outputs, labels)\n            \n            val_loss += loss.item()\n            val_preds.append(F.softmax(outputs, dim=1).cpu().numpy())\n            val_labels.append(labels.cpu().numpy())\n\n    val_preds = np.concatenate(val_preds)\n    val_labels = np.concatenate(val_labels)\n    val_loss /= len(val_loader)\n    train_loss /= len(train_loader)\n    print(f'Epoch: {epoch+1}/{epochs}, Training Loss: {train_loss}, Validation Loss: {val_loss}') #,\n\n    # Calculate metrics\n    val_preds_class = np.argmax(val_preds, axis=1)\n    accuracy = accuracy_score(val_labels, val_preds_class)\n    recall = recall_score(val_labels, val_preds_class, average='weighted')\n    precision = precision_score(val_labels, val_preds_class, average='weighted')\n    f1 = f1_score(val_labels, val_preds_class, average='weighted')\n    micro_f1 = f1_score(val_labels, val_preds_class, average='micro')\n    macro_roc_auc = roc_auc_score(val_labels, val_preds, multi_class='ovo', average='macro')\n    print(f'Accuracy: {accuracy}, Recall: {recall}, Precision: {precision}, F1: {f1}, Micro F1: {micro_f1}, Macro Roc Auc: {macro_roc_auc}')\n\n    # Implement early stopping\n    if macro_roc_auc - best_roc_auc < min_delta:\n        early_stopping_count += 1\n        print(f'EarlyStopping counter: {early_stopping_count} out of {early_stopping_patience}')\n        if early_stopping_count >= early_stopping_patience:\n            print('Early stopping')\n            break\n    else:\n        best_roc_auc = macro_roc_auc\n        early_stopping_count = 0\n        torch.save(ensemble_model.state_dict(), f\"{ensemble_name}_epoch_{epoch}roc_{best_roc_auc}.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-09-19T18:20:08.759337Z","iopub.execute_input":"2024-09-19T18:20:08.759711Z","iopub.status.idle":"2024-09-19T18:27:00.568093Z","shell.execute_reply.started":"2024-09-19T18:20:08.759673Z","shell.execute_reply":"2024-09-19T18:27:00.566381Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model evaluation on Test Dataset","metadata":{}},{"cell_type":"code","source":"#ensemble_model.to('cpu')\n#device = 'cpu'\n\n# Put the model in evaluation mode\nensemble_model.eval()\n\ntest_preds = []\ntest_labels = []\n\n# Iterate over test data\npara_test_loader = pl.MpDeviceLoader(test_loader, device)\nwith torch.no_grad():\n    for batch in tqdm(para_test_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        outputs = ensemble_model(input_ids, attention_mask)\n        test_preds.append(F.softmax(outputs, dim=1).cpu().numpy())\n        test_labels.append(labels.cpu().numpy())\ntest_preds = np.concatenate(test_preds)\ntest_labels = np.concatenate(test_labels)\n\n# Calculate metrics\ntest_preds_class = np.argmax(test_preds, axis=1)\naccuracy = accuracy_score(test_labels, test_preds_class)\nrecall = recall_score(test_labels, test_preds_class, average='weighted')\nprecision = precision_score(test_labels, test_preds_class, average='weighted')\nf1 = f1_score(test_labels, test_preds_class, average='weighted')\nmicro_f1 = f1_score(test_labels, test_preds_class, average='micro')\nmacro_roc_auc = roc_auc_score(test_labels, test_preds, multi_class='ovo', average='macro')\n\nprint(f'Accuracy: {accuracy}, Recall: {recall}, Precision: {precision}, F1: {f1}, Micro F1: {micro_f1}, Macro Roc Auc: {macro_roc_auc}')","metadata":{"execution":{"iopub.status.busy":"2024-09-19T21:09:02.941066Z","iopub.execute_input":"2024-09-19T21:09:02.941528Z","iopub.status.idle":"2024-09-19T22:14:41.248060Z","shell.execute_reply.started":"2024-09-19T21:09:02.941479Z","shell.execute_reply":"2024-09-19T22:14:41.246968Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"100%|██████████| 1100/1100 [1:05:36<00:00,  3.58s/it]","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.4510628623394339, Recall: 0.4510628623394339, Precision: 0.4570243218028462, F1: 0.4405490711681504, Micro F1: 0.4510628623394339, Macro Roc Auc: 0.7177836144007887\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}